zookeeper adminServer URL:
	http://localhost:8080/commands
need to enable in zookeeper.properties


$ bin/zookeeper-server-start.sh ./config/zookeeper.properties
$ bin/kafka-server-start.sh ./config/server.properties
# default broker config for time semantics is log.message.timestamp.type=CreateTime -> this is "event" time and refers to the timestamp in the ProducerRecord


$ bin/kafka-topics.sh --create \
    --bootstrap-server localhost:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic bbg-approved

$ bin/kafka-topics.sh --create \
    --bootstrap-server localhost:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic bbg-cancel

$ bin/kafka-topics.sh --create \
    --bootstrap-server localhost:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic cache-operations \
    --config cleanup.policy=compact

$ bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe

$ bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic bbg-approved --producer-property enable.idempotence=true  --request-required-acks all --property "parse.key=true" --property "key.separator=|"
>{"_t":"tk","tokenId":"123","type":"EventToken","entity":"Bloomberg"}|{"_t":"as","token":{"_t":"tk","tokenId":"123","type":"EventToken","entity":"Bloomberg"},"approvalDetails":[{"_t":"ad","allocId":"123_a","entity":"Bloomberg","taserApprovalId":"taser_123_a","economics":"some_details","marked":false},{"_t":"ad","allocId":"123_b","entity":"Bloomberg","taserApprovalId":"taser_123_b","economics":"some_details_special","marked":false},{"_t":"ad","allocId":"123_c","entity":"Bloomberg","taserApprovalId":"taser_123_c","economics":"some_details","marked":false}]}
>{"_t":"tk","tokenId":"888","type":"EventToken","entity":"Bloomberg"}|{"_t":"as","token":{"_t":"tk","tokenId":"888","type":"EventToken","entity":"Bloomberg"},"approvalDetails":[{"_t":"ad","allocId":"888_a","entity":"Bloomberg","taserApprovalId":"taser_888_a","economics":"some_details","marked":false},{"_t":"ad","allocId":"888_b","entity":"Bloomberg","taserApprovalId":"taser_888_b","economics":"some_details","marked":false}]}

$ bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic bbg-cancel --producer-property enable.idempotence=true  --request-required-acks all --property "parse.key=true" --property "key.separator=|"
>{"_t":"tk","tokenId":"123","type":"EventToken","entity":"Bloomberg"}|{"_t": "ac"}

$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 \
    --topic cache-operations \
    --formatter kafka.tools.DefaultMessageFormatter \
    --property print.key=true \
    --property print.value=true \
    --isolation-level read_committed \
    --from-beginning

# reset Streams application state
$ rm -fr /tmp/kafka-streams/approval-cache-processor/

# reset kakfa brokers
$ rm -fr /tmp/zookeeper/
$ rm -fr /tmp/kafka-logs/


# delete topic
$ bin/kafka-topics.sh --zookeeper localhost:2181 --topic cache-operations --delete
  OR
$ bin/zookeeper-shell.sh localhost:2181  # this is interactive shell for zookeeper
get /brokers/ids/0
ls /brokers/topics
deleteall /brokers/topics/yourtopic
# need to restart kafka to take effect since you are deleting on zookeeper directly


#---- ABOUT "time" IN Kafka Streams ----#
# the default Kafka Streams extractor is "FailOnInvalidTimestamp" which retrieves the built-in timestamp that is automatically embedded into Kafka message by the Kafka producer client (either "CreateTime" or "LogAppendTime" as in the broker config "log.message.timestamp.type")
# for details, read:
# https://kafka.apache.org/28/documentation/streams/developer-guide/config-streams.html#timestamp-extractor
# https://kafka.apache.org/28/javadoc/org/apache/kafka/streams/processor/FailOnInvalidTimestamp.html
# https://kafka.apache.org/28/javadoc/org/apache/kafka/clients/producer/ProducerRecord.html


# GlobalStateManagerImpl.restoreState compares offset (local) and highWatermark (broker) checkpoints, sometimes during debugging this can get out of sync and the GlobalStreamThread will never be able to get into RUNNING state.
If this happens, manually edit the local offset checkpoint file to be consistent with highWatermark.
$ vi /tmp/kafka-streams/approval-cache-processor/global/.checkpoint


Eclipse IDE Debug Shell to check the content of the store:
    org.apache.kafka.streams.state.KeyValueIterator itr = store.all(); while (itr.hasNext()) System.out.println(itr.next());

    store.get(new Token("789", TokenType.AllocToken, Entity.Bloomberg))

    store.put(new Token("789", TokenType.AllocToken, Entity.Bloomberg), ValueAndTimestamp.make(new ApprovalDetails("789_sean_hack", TokenType.AllocToken, Entity.Bloomberg, null), org.apache.kafka.common.utils.Time.SYSTEM.milliseconds()))


# Swagger UI link:
http://localhost:8090/swagger-ui/    (need the slash at the end!)


#--- GRADLE STUFF ---#

# to refresh dependency in Eclipse IDE, right click on project -> Gradle -> Refresh Gradle Project

$ gradle clean build
$ gradle bootRun

# running the application with gradle doesn't work that well because it cannot gracefully shutdown (more below), so install Spring Boot Dashboard in Eclipse and use that

# shutdown hooks are not called when run with "gradle bootRun" and Ctrl-C, hence Streams application may not close gracefully
# someone mentioned the problem may be the gradle daemon - so need to run gradle command without daemon
# https://github.com/gradle/gradle/issues/11517#issuecomment-602632296
#
$ export GRADLE_OPTS="-XX:MaxMetaspaceSize=256m -XX:+HeapDumpOnOutOfMemoryError -Xms256m -Xmx512m -Dfile.encoding=UTF-8 -Duser.country=US -Duser.language=en -Duser.variant"
$ gradle bootRun --no-daemon
#
# however even this still doesn't work! Not sure why. I have to find the pid and use "kill 12345" or "kill -2 12345" command

$ gradle --stop    # stop daemon
$ gradle --status  # check daemon status



#--- MAVEN STUFF ---# this is for old project. New Spring Boot based project uses gradle.
# may need to temporarily set JAVA_HOME to 1.8 version
$ export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64

# CREATE MAVEN project -- this will create a dir named with the specified artifactId
$ mvn archetype:generate \
            -DarchetypeGroupId=org.apache.kafka \
            -DarchetypeArtifactId=streams-quickstart-java \
            -DarchetypeVersion=2.8.0 \
            -DgroupId=sean \
            -DartifactId=sean_kafka_streams_poc \
            -Dversion=0.1 \
            -Dpackage=sean
$ mvn eclipse:eclipse  # this generates/updates .classpath/.project files, so if you need to update 
                       # EclipseIDE classpath after updating pom.xml, re-run this task
# can import into eclipse now

$ mvn clean package
$ mvn exec:java -Dexec.mainClass=sean.RetractApproval


QUESTIONS:
0. should server really handle updating the cache? why can't client do it?
   if server, how long should server wait before checking the store after producing the update message? currently every 10ms
              do we really need to modify both event-level and alloc-level cache entry?

1. configurations
    message retention period for different topics? 1 week? how much disk space will it use?
    max message size?
    how long should the join window be? 1 hour?
    what time semantics to use? event time?
    use avro serialization?

2. where to store zookeeper/kafka broker/kafka streams app logs? local disk or NFS?

3. how to restart streams app processing from a certain offset?

4. how to deploy? one streams app per SEF, plus one for global table?

5. how to write testcases for Kafka Streams

6. what failure scenarios to test for?
   invalid message - how to remove that message?
   crash streams app
   crash kafka broker node - does firm have farm? how reliable?
   crash zookeeper node - does firm have farm? how reliable?

7. test throughput and latency?

8. what is the best way to monitor zookeeper/kafka broker/kafka streams app health? 
   use proper monitoring tools, or watch logs for exceptions?
   seen this error once due to POOR exception handling:
      "All stream threads have died. The instance will be in error state and should be closed.""

9. how to manipulate the store at runtime? JMX or REST?

10. can we deploy this solution to run in parallel with current?

12. can Triclear leverage this? compare with camel and spring integrations, which both have persistent stores for EIP...
