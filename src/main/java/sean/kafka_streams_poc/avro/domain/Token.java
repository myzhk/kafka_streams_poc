/**
 * Autogenerated by Avro
 *
 * DO NOT EDIT DIRECTLY
 */
package sean.kafka_streams_poc.avro.domain;

import org.apache.avro.generic.GenericArray;
import org.apache.avro.specific.SpecificData;
import org.apache.avro.util.Utf8;
import org.apache.avro.message.BinaryMessageEncoder;
import org.apache.avro.message.BinaryMessageDecoder;
import org.apache.avro.message.SchemaStore;

@org.apache.avro.specific.AvroGenerated
public class Token extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  private static final long serialVersionUID = -211950164814281359L;
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Token\",\"namespace\":\"sean.kafka_streams_poc.avro.domain\",\"fields\":[{\"name\":\"tokenId\",\"type\":\"string\"},{\"name\":\"type\",\"type\":{\"type\":\"enum\",\"name\":\"TokenType\",\"symbols\":[\"EventToken\",\"AllocToken\"]}},{\"name\":\"entity\",\"type\":{\"type\":\"enum\",\"name\":\"Entity\",\"symbols\":[\"Bloomberg\",\"TradeWeb\",\"Traiana\"]}}]}");
  public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; }

  private static SpecificData MODEL$ = new SpecificData();

  private static final BinaryMessageEncoder<Token> ENCODER =
      new BinaryMessageEncoder<Token>(MODEL$, SCHEMA$);

  private static final BinaryMessageDecoder<Token> DECODER =
      new BinaryMessageDecoder<Token>(MODEL$, SCHEMA$);

  /**
   * Return the BinaryMessageEncoder instance used by this class.
   * @return the message encoder used by this class
   */
  public static BinaryMessageEncoder<Token> getEncoder() {
    return ENCODER;
  }

  /**
   * Return the BinaryMessageDecoder instance used by this class.
   * @return the message decoder used by this class
   */
  public static BinaryMessageDecoder<Token> getDecoder() {
    return DECODER;
  }

  /**
   * Create a new BinaryMessageDecoder instance for this class that uses the specified {@link SchemaStore}.
   * @param resolver a {@link SchemaStore} used to find schemas by fingerprint
   * @return a BinaryMessageDecoder instance for this class backed by the given SchemaStore
   */
  public static BinaryMessageDecoder<Token> createDecoder(SchemaStore resolver) {
    return new BinaryMessageDecoder<Token>(MODEL$, SCHEMA$, resolver);
  }

  /**
   * Serializes this Token to a ByteBuffer.
   * @return a buffer holding the serialized data for this instance
   * @throws java.io.IOException if this instance could not be serialized
   */
  public java.nio.ByteBuffer toByteBuffer() throws java.io.IOException {
    return ENCODER.encode(this);
  }

  /**
   * Deserializes a Token from a ByteBuffer.
   * @param b a byte buffer holding serialized data for an instance of this class
   * @return a Token instance decoded from the given buffer
   * @throws java.io.IOException if the given bytes could not be deserialized into an instance of this class
   */
  public static Token fromByteBuffer(
      java.nio.ByteBuffer b) throws java.io.IOException {
    return DECODER.decode(b);
  }

   private java.lang.CharSequence tokenId;
   private sean.kafka_streams_poc.avro.domain.TokenType type;
   private sean.kafka_streams_poc.avro.domain.Entity entity;

  /**
   * Default constructor.  Note that this does not initialize fields
   * to their default values from the schema.  If that is desired then
   * one should use <code>newBuilder()</code>.
   */
  public Token() {}

  /**
   * All-args constructor.
   * @param tokenId The new value for tokenId
   * @param type The new value for type
   * @param entity The new value for entity
   */
  public Token(java.lang.CharSequence tokenId, sean.kafka_streams_poc.avro.domain.TokenType type, sean.kafka_streams_poc.avro.domain.Entity entity) {
    this.tokenId = tokenId;
    this.type = type;
    this.entity = entity;
  }

  public org.apache.avro.specific.SpecificData getSpecificData() { return MODEL$; }
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call.
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return tokenId;
    case 1: return type;
    case 2: return entity;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }

  // Used by DatumReader.  Applications should not call.
  @SuppressWarnings(value="unchecked")
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: tokenId = (java.lang.CharSequence)value$; break;
    case 1: type = (sean.kafka_streams_poc.avro.domain.TokenType)value$; break;
    case 2: entity = (sean.kafka_streams_poc.avro.domain.Entity)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }

  /**
   * Gets the value of the 'tokenId' field.
   * @return The value of the 'tokenId' field.
   */
  public java.lang.CharSequence getTokenId() {
    return tokenId;
  }


  /**
   * Sets the value of the 'tokenId' field.
   * @param value the value to set.
   */
  public void setTokenId(java.lang.CharSequence value) {
    this.tokenId = value;
  }

  /**
   * Gets the value of the 'type' field.
   * @return The value of the 'type' field.
   */
  public sean.kafka_streams_poc.avro.domain.TokenType getType() {
    return type;
  }


  /**
   * Sets the value of the 'type' field.
   * @param value the value to set.
   */
  public void setType(sean.kafka_streams_poc.avro.domain.TokenType value) {
    this.type = value;
  }

  /**
   * Gets the value of the 'entity' field.
   * @return The value of the 'entity' field.
   */
  public sean.kafka_streams_poc.avro.domain.Entity getEntity() {
    return entity;
  }


  /**
   * Sets the value of the 'entity' field.
   * @param value the value to set.
   */
  public void setEntity(sean.kafka_streams_poc.avro.domain.Entity value) {
    this.entity = value;
  }

  /**
   * Creates a new Token RecordBuilder.
   * @return A new Token RecordBuilder
   */
  public static sean.kafka_streams_poc.avro.domain.Token.Builder newBuilder() {
    return new sean.kafka_streams_poc.avro.domain.Token.Builder();
  }

  /**
   * Creates a new Token RecordBuilder by copying an existing Builder.
   * @param other The existing builder to copy.
   * @return A new Token RecordBuilder
   */
  public static sean.kafka_streams_poc.avro.domain.Token.Builder newBuilder(sean.kafka_streams_poc.avro.domain.Token.Builder other) {
    if (other == null) {
      return new sean.kafka_streams_poc.avro.domain.Token.Builder();
    } else {
      return new sean.kafka_streams_poc.avro.domain.Token.Builder(other);
    }
  }

  /**
   * Creates a new Token RecordBuilder by copying an existing Token instance.
   * @param other The existing instance to copy.
   * @return A new Token RecordBuilder
   */
  public static sean.kafka_streams_poc.avro.domain.Token.Builder newBuilder(sean.kafka_streams_poc.avro.domain.Token other) {
    if (other == null) {
      return new sean.kafka_streams_poc.avro.domain.Token.Builder();
    } else {
      return new sean.kafka_streams_poc.avro.domain.Token.Builder(other);
    }
  }

  /**
   * RecordBuilder for Token instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Token>
    implements org.apache.avro.data.RecordBuilder<Token> {

    private java.lang.CharSequence tokenId;
    private sean.kafka_streams_poc.avro.domain.TokenType type;
    private sean.kafka_streams_poc.avro.domain.Entity entity;

    /** Creates a new Builder */
    private Builder() {
      super(SCHEMA$);
    }

    /**
     * Creates a Builder by copying an existing Builder.
     * @param other The existing Builder to copy.
     */
    private Builder(sean.kafka_streams_poc.avro.domain.Token.Builder other) {
      super(other);
      if (isValidValue(fields()[0], other.tokenId)) {
        this.tokenId = data().deepCopy(fields()[0].schema(), other.tokenId);
        fieldSetFlags()[0] = other.fieldSetFlags()[0];
      }
      if (isValidValue(fields()[1], other.type)) {
        this.type = data().deepCopy(fields()[1].schema(), other.type);
        fieldSetFlags()[1] = other.fieldSetFlags()[1];
      }
      if (isValidValue(fields()[2], other.entity)) {
        this.entity = data().deepCopy(fields()[2].schema(), other.entity);
        fieldSetFlags()[2] = other.fieldSetFlags()[2];
      }
    }

    /**
     * Creates a Builder by copying an existing Token instance
     * @param other The existing instance to copy.
     */
    private Builder(sean.kafka_streams_poc.avro.domain.Token other) {
      super(SCHEMA$);
      if (isValidValue(fields()[0], other.tokenId)) {
        this.tokenId = data().deepCopy(fields()[0].schema(), other.tokenId);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.type)) {
        this.type = data().deepCopy(fields()[1].schema(), other.type);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.entity)) {
        this.entity = data().deepCopy(fields()[2].schema(), other.entity);
        fieldSetFlags()[2] = true;
      }
    }

    /**
      * Gets the value of the 'tokenId' field.
      * @return The value.
      */
    public java.lang.CharSequence getTokenId() {
      return tokenId;
    }


    /**
      * Sets the value of the 'tokenId' field.
      * @param value The value of 'tokenId'.
      * @return This builder.
      */
    public sean.kafka_streams_poc.avro.domain.Token.Builder setTokenId(java.lang.CharSequence value) {
      validate(fields()[0], value);
      this.tokenId = value;
      fieldSetFlags()[0] = true;
      return this;
    }

    /**
      * Checks whether the 'tokenId' field has been set.
      * @return True if the 'tokenId' field has been set, false otherwise.
      */
    public boolean hasTokenId() {
      return fieldSetFlags()[0];
    }


    /**
      * Clears the value of the 'tokenId' field.
      * @return This builder.
      */
    public sean.kafka_streams_poc.avro.domain.Token.Builder clearTokenId() {
      tokenId = null;
      fieldSetFlags()[0] = false;
      return this;
    }

    /**
      * Gets the value of the 'type' field.
      * @return The value.
      */
    public sean.kafka_streams_poc.avro.domain.TokenType getType() {
      return type;
    }


    /**
      * Sets the value of the 'type' field.
      * @param value The value of 'type'.
      * @return This builder.
      */
    public sean.kafka_streams_poc.avro.domain.Token.Builder setType(sean.kafka_streams_poc.avro.domain.TokenType value) {
      validate(fields()[1], value);
      this.type = value;
      fieldSetFlags()[1] = true;
      return this;
    }

    /**
      * Checks whether the 'type' field has been set.
      * @return True if the 'type' field has been set, false otherwise.
      */
    public boolean hasType() {
      return fieldSetFlags()[1];
    }


    /**
      * Clears the value of the 'type' field.
      * @return This builder.
      */
    public sean.kafka_streams_poc.avro.domain.Token.Builder clearType() {
      type = null;
      fieldSetFlags()[1] = false;
      return this;
    }

    /**
      * Gets the value of the 'entity' field.
      * @return The value.
      */
    public sean.kafka_streams_poc.avro.domain.Entity getEntity() {
      return entity;
    }


    /**
      * Sets the value of the 'entity' field.
      * @param value The value of 'entity'.
      * @return This builder.
      */
    public sean.kafka_streams_poc.avro.domain.Token.Builder setEntity(sean.kafka_streams_poc.avro.domain.Entity value) {
      validate(fields()[2], value);
      this.entity = value;
      fieldSetFlags()[2] = true;
      return this;
    }

    /**
      * Checks whether the 'entity' field has been set.
      * @return True if the 'entity' field has been set, false otherwise.
      */
    public boolean hasEntity() {
      return fieldSetFlags()[2];
    }


    /**
      * Clears the value of the 'entity' field.
      * @return This builder.
      */
    public sean.kafka_streams_poc.avro.domain.Token.Builder clearEntity() {
      entity = null;
      fieldSetFlags()[2] = false;
      return this;
    }

    @Override
    @SuppressWarnings("unchecked")
    public Token build() {
      try {
        Token record = new Token();
        record.tokenId = fieldSetFlags()[0] ? this.tokenId : (java.lang.CharSequence) defaultValue(fields()[0]);
        record.type = fieldSetFlags()[1] ? this.type : (sean.kafka_streams_poc.avro.domain.TokenType) defaultValue(fields()[1]);
        record.entity = fieldSetFlags()[2] ? this.entity : (sean.kafka_streams_poc.avro.domain.Entity) defaultValue(fields()[2]);
        return record;
      } catch (org.apache.avro.AvroMissingFieldException e) {
        throw e;
      } catch (java.lang.Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
    }
  }

  @SuppressWarnings("unchecked")
  private static final org.apache.avro.io.DatumWriter<Token>
    WRITER$ = (org.apache.avro.io.DatumWriter<Token>)MODEL$.createDatumWriter(SCHEMA$);

  public static Token fromToken(sean.kafka_streams_poc.domain.Token token) {
    Token ret = new Token();
    ret.tokenId = token.tokenId;
    ret.entity = Entity.valueOf(token.entity.name());
    ret.type = TokenType.valueOf(token.type.name());

    return ret;
  }

  @Override public void writeExternal(java.io.ObjectOutput out)
    throws java.io.IOException {
    WRITER$.write(this, SpecificData.getEncoder(out));
  }

  @SuppressWarnings("unchecked")
  private static final org.apache.avro.io.DatumReader<Token>
    READER$ = (org.apache.avro.io.DatumReader<Token>)MODEL$.createDatumReader(SCHEMA$);

  @Override public void readExternal(java.io.ObjectInput in)
    throws java.io.IOException {
    READER$.read(this, SpecificData.getDecoder(in));
  }

  @Override protected boolean hasCustomCoders() { return true; }

  @Override public void customEncode(org.apache.avro.io.Encoder out)
    throws java.io.IOException
  {
    out.writeString(this.tokenId);

    out.writeEnum(this.type.ordinal());

    out.writeEnum(this.entity.ordinal());

  }

  @Override public void customDecode(org.apache.avro.io.ResolvingDecoder in)
    throws java.io.IOException
  {
    org.apache.avro.Schema.Field[] fieldOrder = in.readFieldOrderIfDiff();
    if (fieldOrder == null) {
      this.tokenId = in.readString(this.tokenId instanceof Utf8 ? (Utf8)this.tokenId : null);

      this.type = sean.kafka_streams_poc.avro.domain.TokenType.values()[in.readEnum()];

      this.entity = sean.kafka_streams_poc.avro.domain.Entity.values()[in.readEnum()];

    } else {
      for (int i = 0; i < 3; i++) {
        switch (fieldOrder[i].pos()) {
        case 0:
          this.tokenId = in.readString(this.tokenId instanceof Utf8 ? (Utf8)this.tokenId : null);
          break;

        case 1:
          this.type = sean.kafka_streams_poc.avro.domain.TokenType.values()[in.readEnum()];
          break;

        case 2:
          this.entity = sean.kafka_streams_poc.avro.domain.Entity.values()[in.readEnum()];
          break;

        default:
          throw new java.io.IOException("Corrupt ResolvingDecoder.");
        }
      }
    }
  }
}










